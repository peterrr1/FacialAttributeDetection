{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from PIL import Image\n",
    "import sys\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "from torchvision.transforms import v2\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from torchvision.transforms.functional import to_pil_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_IMAGES = 'data/img_align_celeba'\n",
    "PATH_TO_LABELS = 'data/list_attr_celeba.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageLoader(Dataset):\n",
    "    def __init__(self, data_path, label_path, img_size=(234, 234), augment=True):\n",
    "\n",
    "        self.data_path = data_path\n",
    "        self.label_path = label_path\n",
    "        self.augment = augment\n",
    "        self.attr_names = self.get_attribute_names_from_csv()\n",
    "        self.distribution = np.zeros(len(self.attr_names))\n",
    "        self.images = self.get_images_from_directory(augment)\n",
    "        self.labels = self.get_labels_from_csv()\n",
    "\n",
    "        self.transform = v2.Compose([\n",
    "            v2.Resize(size=img_size),\n",
    "            #v2.CenterCrop(224),\n",
    "            v2.ToImage(),\n",
    "            v2.ToDtype(torch.float32, scale=True),\n",
    "            # Normalization for pretrained mobilenet: mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225]\n",
    "            #v2.Normalize(mean=[0.5, 0.5, 0.5],std=[0.5, 0.5, 0.5])\n",
    "        ])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.images[idx])\n",
    "        image_tensor = self.transform(image)\n",
    "        label_tensor = torch.Tensor(self.labels[int(idx / 4)]) if self.augment else torch.Tensor(self.labels[idx])\n",
    "        return image_tensor, label_tensor\n",
    "\n",
    "    def get_distribution(self):\n",
    "        return self.distribution\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def get_images_from_directory(self, augment):\n",
    "        if augment:\n",
    "            return self.augment_images()\n",
    "        else:\n",
    "            return sorted(glob.glob(f'{self.data_path}/*.jpg'))\n",
    "\n",
    "    def get_labels_from_csv(self):\n",
    "        label_list = open(self.label_path).readlines()[1:]\n",
    "        data_label = []\n",
    "        for i in range(len(label_list)):\n",
    "            data_label.append(label_list[i].strip().split(',')[1:])\n",
    "        for i in range(len(data_label)):\n",
    "            data_label[i] = [j.replace('-1', '0') for j in data_label[i]]\n",
    "            data_label[i] = [int(j) for j in data_label[i]]\n",
    "            self.distribution += np.array(data_label[i])\n",
    "        return data_label\n",
    "\n",
    "    def get_attribute_names_from_csv(self):\n",
    "        return open(self.label_path).readlines()[0].split(',')[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageLoader(PATH_TO_IMAGES, PATH_TO_LABELS, img_size=(144, 144),augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(images, num_of_samples=10, cols=5):\n",
    "    images = iter(dataset)\n",
    "    plt.figure(figsize=(17, 17))\n",
    "    for i in range(num_of_samples):\n",
    "        img, _ = next(images)\n",
    "        plt.subplot(int(num_of_samples / cols) + 1, cols, i + 1)\n",
    "        plt.imshow(to_pil_image(img))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial shape:  torch.Size([1, 3, 144, 144])\n",
      "Patches shape:  torch.Size([1, 324, 128])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from einops.layers.torch import Rearrange\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels = 3, patch_size = 8, emb_size = 128):\n",
    "        self.patch_size = patch_size\n",
    "        super().__init__()\n",
    "        self.projection = nn.Sequential(\n",
    "            # break-down the image in s1 x s2 patches and flat them\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=patch_size, p2=patch_size),\n",
    "            nn.Linear(patch_size * patch_size * in_channels, emb_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.projection(x)\n",
    "        return x\n",
    "\n",
    "# Run a quick test\n",
    "sample_datapoint = torch.unsqueeze(dataset[0][0], 0)\n",
    "print(\"Initial shape: \", sample_datapoint.shape)\n",
    "embedding = PatchEmbedding()(sample_datapoint)\n",
    "print(\"Patches shape: \", embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, n_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.att = torch.nn.MultiheadAttention(embed_dim=dim,\n",
    "                                               num_heads=n_heads,\n",
    "                                               dropout=dropout)\n",
    "        self.q = torch.nn.Linear(dim, dim)\n",
    "        self.k = torch.nn.Linear(dim, dim)\n",
    "        self.v = torch.nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        q = self.q(x)\n",
    "        k = self.k(x)\n",
    "        v = self.v(x)\n",
    "        attn_output, attn_output_weights = self.att(q, k, v)\n",
    "        return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 128])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Attention(dim=128, n_heads=4, dropout=0.)(torch.ones((1, 5, 128))).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "face_recognition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
