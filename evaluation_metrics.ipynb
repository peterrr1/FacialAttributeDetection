{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from input_pipeline import ImageLoader\n",
    "from sklearn import metrics\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "PATH_TO_IMAGES = 'data/img_align_celeba'\n",
    "PATH_TO_LABELS = 'data/list_attr_celeba.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageLoader(PATH_TO_IMAGES, PATH_TO_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_from_csv(path):\n",
    "        label_list = open(path).readlines()[1:]\n",
    "        data_label = []\n",
    "        for i in range(len(label_list)):\n",
    "            data_label.append(label_list[i].strip().split(',')[1:])\n",
    "        for i in range(len(data_label)):\n",
    "            data_label[i] = [j.replace('-1', '0') for j in data_label[i]]\n",
    "            data_label[i] = [int(j) for j in data_label[i]]\n",
    "        return data_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = open(PATH_TO_LABELS).readlines()[0].split(',')[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dummy data and use the first 100 labels from the actual dataset\n",
    "dummy_labels = torch.Tensor(get_labels_from_csv(PATH_TO_LABELS)[0:100])\n",
    "random_tensors = torch.randn(size=(100, 40))\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "outputs = sigmoid(random_tensors)\n",
    "tau_threshold = 0.5\n",
    "dummy_data = (outputs > tau_threshold).int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array([[1, 0, 0], [1, 1, 1]])\n",
    "y_pred = np.array([[1, 1, 0], [1, 1, 0]])\n",
    "\n",
    "def hamming_score(y_true, y_pred):\n",
    "    acc_list = []\n",
    "    for i in range(y_true.shape[0]):\n",
    "        set_true = set(np.where(y_true[i])[0])\n",
    "        set_pred = set(np.where(y_pred[i])[0])\n",
    "\n",
    "        tmp_a = None\n",
    "        if len(set_true) == 0 and len(set_pred) == 0:\n",
    "            tmp_a = 1\n",
    "        else:\n",
    "            tmp_a = len(set_true.intersection(set_pred)) / float(len(set_true.union(set_pred)))\n",
    "        acc_list.append(tmp_a)\n",
    "    return np.mean(acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## F1 scores for multilabel\n",
    "## average=None means that f1 score is calcualted for each class\n",
    "\n",
    "y_true = np.array([[0, 0, 0], [1, 1, 1], [1, 1, 1]])\n",
    "y_pred = np.array([[0, 1, 0], [1, 1, 1], [1, 1, 1]])\n",
    "## average='samples': Calculate metrics for each instance, and find their average \n",
    "## (only meaningful for multilabel classification where this differs from accuracy_score\n",
    "def calc_evaluation_metrics(y_true, y_pred, epoch, batch):\n",
    "    print('Epoch: {}, Batch: {}'.format(epoch + 1, batch + 1))\n",
    "    print('F1 score:')\n",
    "    print(metrics.f1_score(y_pred=y_pred, y_true=y_true, average='samples'))\n",
    "\n",
    "    print('Recall:')\n",
    "    print(metrics.recall_score(y_pred=y_pred, y_true=y_true, average='samples'))\n",
    "\n",
    "    print('Precision:')\n",
    "    print(metrics.precision_score(y_pred=y_pred, y_true=y_true, average='samples'))\n",
    "\n",
    "    print('Exact accuracy:')\n",
    "    print(metrics.accuracy_score(y_pred=y_pred, y_true=y_true))\n",
    "\n",
    "    print('Hamming loss:')\n",
    "    print(metrics.hamming_loss(y_pred=y_pred, y_true=y_true))\n",
    "\n",
    "    print('Hamming score:')\n",
    "    print(hamming_score(y_true, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_true = np.array([0, 1, 0])\n",
    "y_pred = np.array([0, 1, 0])\n",
    "\n",
    "mx = metrics.multilabel_confusion_matrix(y_true=dummy_labels, y_pred=dummy_data, samplewise=True)\n",
    "\n",
    "\n",
    "for m in range(100):\n",
    "    plt.imshow(mx[m], cmap='summer',interpolation='nearest')\n",
    "    plt.colorbar()\n",
    "    plt.xticks(np.arange(2), ['0', '1'])\n",
    "    plt.yticks(np.arange(2), ['0', '1'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(f'Confusion Matrix for instance {m+1}')\n",
    "\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            plt.text(j, i, mx[m, i, j], ha='center', va='center', color='black')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_1 = np.array([[0, 1, 0], [1, 1, 0]])\n",
    "y_pred_1 = np.array([[0, 1, 0], [1, 0, 1]])\n",
    "label_accuracy_1 = np.mean(y_true_1 == y_pred_1, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_2 = np.array([[0, 1, 0], [1, 1, 0]])\n",
    "y_pred_2 = np.array([[0, 0, 0], [1, 0, 1]])\n",
    "print(y_pred_2.shape)\n",
    "label_accuracy_2 = np.mean(y_true_2 == y_pred_2, axis=0)\n",
    "print(label_accuracy_1)\n",
    "print(label_accuracy_2)\n",
    "overall_accuracy = (label_accuracy_1 + label_accuracy_2) / 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multilabel classification\n",
    "y_true = [[0, 0, 0], [1, 1, 1], [0, 1, 1], [1, 0, 0]]\n",
    "y_pred = [[0, 0, 0], [1, 1, 1], [1, 1, 0], [0, 1, 1]]\n",
    "metrics.hamming_loss(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_eval_scores(f1_score, recall, precision, hamming_loss, ham_score, partial_accuracy, label_wise_accuracy, epoch='Avg', batch='Avg', ):\n",
    "    print('\\n======================== Epoch: {}, Batch: {} ========================'.format(epoch, batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================== Epoch: 1, Batch: 0 ========================\n"
     ]
    }
   ],
   "source": [
    "print_eval_scores(10, 10, 10, 10, 10, 10, 10, 0 + 1, 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "face_recognition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
