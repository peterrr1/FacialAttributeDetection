{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import torchvision\n",
    "from dataloader import CelebA\n",
    "from data_manager.manage_csv import *\n",
    "import pandas as pd\n",
    "from evaluation_metrics import eval_scores, print_eval_scores\n",
    "from dataloader import create_dataset_split\n",
    "import numpy as np\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Define batch size, number of epochs for training and dataset paths \"\"\"\n",
    "\n",
    "PATH_TO_IMAGES = 'data/img_align_celeba'\n",
    "PATH_TO_LABELS = 'data/list_attr_celeba.csv'\n",
    "PATH_TO_MODELS = 'models/'\n",
    "PATH_TO_VALIDATION_SCORES = 'metadata/validation_scores.csv'\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 2\n",
    "ADAPTIVE_THRESHOLD_RATE = 1.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Load pretrained model and dataset \"\"\" \n",
    "\n",
    "#model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', weights=torchvision.models.MobileNet_V2_Weights.IMAGENET1K_V2)\n",
    "dataset = CelebA(PATH_TO_IMAGES, PATH_TO_LABELS, augment=False)\n",
    "train_data, val_data, test_data = create_dataset_split(dataset=dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_SIZE = len(val_data) * BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MobileNet(nn.Module):\n",
    "    def __init__(self, classifier):\n",
    "        super(MobileNet, self).__init__()\n",
    "        self.model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', weights=torchvision.models.MobileNet_V2_Weights.IMAGENET1K_V2)\n",
    "        self.model.classifier = classifier\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Create classifier for the core model \"\"\"\n",
    "\n",
    "classifier_1 = torch.nn.Sequential(\n",
    "    torch.nn.Dropout(0.3),\n",
    "    torch.nn.Linear(1280, 128),\n",
    "    torch.nn.BatchNorm1d(128),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(0.2),\n",
    "    torch.nn.Linear(128, 64),\n",
    "    torch.nn.BatchNorm1d(64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(0.1),\n",
    "    torch.nn.Linear(64, 40)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_2 = torch.nn.Sequential(\n",
    "    torch.nn.Dropout(0.3),\n",
    "    torch.nn.Linear(1280, 128),\n",
    "    torch.nn.BatchNorm1d(128),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(0.2),\n",
    "    torch.nn.Linear(128, 40)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_3 = torch.nn.Sequential(\n",
    "    torch.nn.Dropout(0.3),\n",
    "    torch.nn.Linear(1280, 40)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/peterbrezovcsik/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    }
   ],
   "source": [
    "model = MobileNet(classifier=classifier_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Define loss function and optimizer \"\"\"\n",
    "\n",
    "loss = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Define model metadata to store in model_metadata.csv \"\"\"\n",
    "\n",
    "VERSION_NUM = 5\n",
    "\n",
    "MODEL_ID = 'MOBILE_NET_V2' + '_' + str(VERSION_NUM)\n",
    "NUM_OF_HEAD_LAYERS = len(model.model.classifier) \n",
    "LOSS_NAME = loss.__class__.__name__\n",
    "OPTI_NAME = optimizer.__class__.__name__\n",
    "LEARNING_RATE = optimizer.defaults['lr']\n",
    "WEIGHT_DECAY = optimizer.defaults['weight_decay']\n",
    "CLASSIFICATION_THRESHOLD = 0.5\n",
    "\n",
    "CLASSIFICATION_THRESHOLD_VECTOR = torch.tensor([CLASSIFICATION_THRESHOLD]*40)\n",
    "\n",
    "\n",
    "\"\"\" Define model metadata file path and header \"\"\"\n",
    "\n",
    "MODEL_METADATA_PATH = 'metadata/'\n",
    "MODEL_METADATA_FILE = 'model_metadata.csv'\n",
    "MODEL_METADATA_HEADER = ['model_id', 'num_of_head_layers', 'batch_size', 'num_epochs', 'loss_fn', 'optimizer', 'learning_rate', 'threshold', 'weight_decay']\n",
    "MODEL_ARGS = [MODEL_ID, NUM_OF_HEAD_LAYERS, BATCH_SIZE, NUM_EPOCHS, LOSS_NAME, OPTI_NAME, LEARNING_RATE, CLASSIFICATION_THRESHOLD, WEIGHT_DECAY]\n",
    "\n",
    "\n",
    "\"\"\" Define class_wise_accuracy file path and header \"\"\"\n",
    "\n",
    "CLASS_WISE_ACCURACY_FILE = 'class_wise_accuracy.csv'\n",
    "CLASS_WISE_ACCURACY_HEADER = dataset.attr_names\n",
    "\n",
    "\n",
    "\"\"\" Define validation scores file path and header \"\"\"\n",
    "\n",
    "VALIDATION_SCORES_FILE = 'validation_scores.csv' \n",
    "VALIDATION_SCORES_HEADER = ['model_id', 'epoch', 'f1_score', 'recall_score', 'precision_score', 'hamming_loss', 'hamming_score', 'partial_accuracy', 'loss']\n",
    "\n",
    "\n",
    "\"\"\" Define threshold values file path and header \"\"\"\n",
    "\n",
    "THRESHOLD_VALUES_FILE = 'threshold_values.csv'\n",
    "THRESHOLD_VALUES_HEADER = dataset.attr_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model_id, current_accuracy):\n",
    "    df = pd.read_csv(PATH_TO_VALIDATION_SCORES)\n",
    "    best_accuracy = df.loc[df['model_id'] == model_id]['partial_accuracy'].max()\n",
    "    if current_accuracy > best_accuracy or best_accuracy is np.nan:\n",
    "        print('Saving model...')\n",
    "        model_path = model_id + '.pth'\n",
    "        path = os.path.join(PATH_TO_MODELS, model_path)\n",
    "        torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fp_fn(y_pred, y_true):\n",
    "\n",
    "    fp = torch.sum((y_pred == 1) & (y_true == 0), axis=0)\n",
    "    fn = torch.sum((y_pred == 0) & (y_true == 1), axis=0)\n",
    "    return fp, fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_threshold(y_pred, y_true, current_epoch, current_threshold):\n",
    "    fp, fn = calculate_fp_fn(y_pred, y_true)\n",
    "    lambda_ = np.power(ADAPTIVE_THRESHOLD_RATE, current_epoch)\n",
    "    nominator = lambda_ * (fp - fn)\n",
    "    new_thres = current_threshold + nominator / VALIDATION_SIZE\n",
    "    return new_thres\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_metadata.csv.csv already exists.\n",
      "class_wise_accuracy.csv already exists.\n",
      "validation_scores.csv already exists.\n",
      "threshold_values.csv already exists.\n",
      "Saving model metadata...\n"
     ]
    }
   ],
   "source": [
    "metric_saver = MetricSaver(MODEL_METADATA_PATH)\n",
    "\n",
    "metric_saver.create_model_metadata_csv(MODEL_METADATA_FILE, MODEL_METADATA_HEADER)\n",
    "metric_saver.create_class_wise_acc_csv(CLASS_WISE_ACCURACY_FILE, *CLASS_WISE_ACCURACY_HEADER)\n",
    "metric_saver.create_validation_scores_csv(VALIDATION_SCORES_FILE, VALIDATION_SCORES_HEADER)\n",
    "metric_saver.create_threshold_values_csv(THRESHOLD_VALUES_FILE, *THRESHOLD_VALUES_HEADER)\n",
    "\n",
    "metric_saver.save_model_metadata(*MODEL_ARGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Validation function for the classifier \"\"\"\n",
    "\n",
    "def validate(model, val_data, epoch, model_id, loss_fn, threshold):\n",
    "    model.eval()\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    avg_partial_accuracy = avg_f1_score = avg_recall_score = avg_precision_score = avg_hamming_loss = avg_hamming_score = avg_loss = 0.0\n",
    "    avg_label_wise_accuracy_score = np.zeros(shape=(40))\n",
    "\n",
    "    NUM_OF_BATCHES = len(val_data)\n",
    "    \n",
    "    current_threshold = threshold\n",
    "\n",
    "    for i, batch in enumerate(val_data):\n",
    "        with torch.no_grad():\n",
    "            images, labels = batch\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            outputs = sigmoid(outputs)\n",
    "\n",
    "            print(current_threshold)\n",
    "            result = outputs > current_threshold\n",
    "\n",
    "            current_threshold = optimize_threshold(result, labels, epoch, current_threshold)\n",
    "           \n",
    "\n",
    "            f1_score, recall, precision, hamming_loss, ham_score, partial_accuracy, label_wise_accuracy =\\\n",
    "                        eval_scores(labels.cpu().detach().numpy(), result.cpu().detach().numpy(), loss.item(), print_out=True, epoch=epoch+1, batch=i)\n",
    "           \n",
    "            ### Increase average scores by the current batch scores\n",
    "            avg_f1_score += f1_score\n",
    "            avg_recall_score += recall\n",
    "            avg_precision_score += precision\n",
    "            avg_hamming_loss += hamming_loss\n",
    "            avg_hamming_score += ham_score\n",
    "            avg_partial_accuracy += partial_accuracy\n",
    "            avg_label_wise_accuracy_score += label_wise_accuracy\n",
    "            avg_loss += loss.item()\n",
    "            \n",
    "    ### Calculate average scores\n",
    "    avg_f1_score /= NUM_OF_BATCHES\n",
    "    avg_recall_score /= NUM_OF_BATCHES\n",
    "    avg_precision_score /= NUM_OF_BATCHES\n",
    "    avg_hamming_loss /= NUM_OF_BATCHES\n",
    "    avg_hamming_score /= NUM_OF_BATCHES\n",
    "    avg_partial_accuracy /= NUM_OF_BATCHES\n",
    "    avg_label_wise_accuracy_score /= NUM_OF_BATCHES\n",
    "    avg_loss /= NUM_OF_BATCHES\n",
    "    \n",
    "\n",
    "    ### Save validation scores to csv file\n",
    "    save_model(model_id, avg_partial_accuracy)\n",
    "    metric_saver.save_class_wise_accuracy(model_id, epoch+1, *avg_label_wise_accuracy_score)\n",
    "    metric_saver.save_validation_scores(model_id, epoch+1, *(avg_f1_score, avg_recall_score, avg_precision_score, avg_hamming_loss, avg_hamming_score, avg_partial_accuracy, avg_loss))\n",
    "    metric_saver.save_threshold_values(model_id, epoch+1, *(current_threshold.detach().numpy()))\n",
    "\n",
    "    print_eval_scores(avg_f1_score, avg_recall_score, avg_precision_score, avg_hamming_loss, avg_hamming_score, avg_partial_accuracy, avg_label_wise_accuracy_score, avg_loss)\n",
    "    print('Threshold values: ', current_threshold)\n",
    "    return current_threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Test function for the classifier \"\"\"\n",
    "\n",
    "def test(model, val_data, model_id, loss_fn, threshold):\n",
    "    model.eval()\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    avg_partial_accuracy = avg_f1_score = avg_recall_score = avg_precision_score = avg_hamming_loss = avg_hamming_score = avg_loss = 0.0\n",
    "    avg_label_wise_accuracy_score = np.zeros(shape=(40))\n",
    "\n",
    "    NUM_OF_BATCHES = len(val_data)\n",
    "    \n",
    "    current_threshold = threshold\n",
    "\n",
    "    for i, batch in enumerate(val_data):\n",
    "        with torch.no_grad():\n",
    "            images, labels = batch\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            outputs = sigmoid(outputs)\n",
    "\n",
    "            result = outputs > current_threshold\n",
    "\n",
    "            f1_score, recall, precision, hamming_loss, ham_score, partial_accuracy, label_wise_accuracy =\\\n",
    "                        eval_scores(labels.cpu().detach().numpy(), result.cpu().detach().numpy(), loss.item(), print_out=True, epoch='Test', batch=i)\n",
    "           \n",
    "            ### Increase average scores by the current batch scores\n",
    "            avg_f1_score += f1_score\n",
    "            avg_recall_score += recall\n",
    "            avg_precision_score += precision\n",
    "            avg_hamming_loss += hamming_loss\n",
    "            avg_hamming_score += ham_score\n",
    "            avg_partial_accuracy += partial_accuracy\n",
    "            avg_label_wise_accuracy_score += label_wise_accuracy\n",
    "            avg_loss += loss.item()\n",
    "            \n",
    "    ### Calculate average scores\n",
    "    avg_f1_score /= NUM_OF_BATCHES\n",
    "    avg_recall_score /= NUM_OF_BATCHES\n",
    "    avg_precision_score /= NUM_OF_BATCHES\n",
    "    avg_hamming_loss /= NUM_OF_BATCHES\n",
    "    avg_hamming_score /= NUM_OF_BATCHES\n",
    "    avg_partial_accuracy /= NUM_OF_BATCHES\n",
    "    avg_label_wise_accuracy_score /= NUM_OF_BATCHES\n",
    "    avg_loss /= NUM_OF_BATCHES\n",
    "    \n",
    "    \"\"\" SAVE TEST SCORES \"\"\"\n",
    "    # TODO: Create a separate function for saving test scores\n",
    "    #metric_saver.save_class_wise_accuracy(model_id, epoch+1, *avg_label_wise_accuracy_score)\n",
    "    #metric_saver.save_validation_scores(model_id, epoch+1, *(avg_f1_score, avg_recall_score, avg_precision_score, avg_hamming_loss, avg_hamming_score, avg_partial_accuracy, avg_loss))\n",
    "    #metric_saver.save_threshold_values(model_id, epoch+1, *(current_threshold.detach().numpy()))\n",
    "\n",
    "    print_eval_scores(avg_f1_score, avg_recall_score, avg_precision_score, avg_hamming_loss, avg_hamming_score, avg_partial_accuracy, avg_label_wise_accuracy_score, avg_loss)\n",
    "    print('Threshold values: ', current_threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Training function for the classifier \"\"\"\n",
    "\n",
    "def fit(model, train_data, val_data, optimizer, loss_fn, epochs, model_id, threshold):\n",
    "        model.train(mode=True)\n",
    "        sigmoid = torch.nn.Sigmoid()\n",
    "        # Froze feature layers\n",
    "        for param in model.model.features.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            for i, batch in enumerate(train_data):\n",
    "                images, labels = batch\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(images)\n",
    "\n",
    "                loss = loss_fn(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                outputs = sigmoid(outputs)\n",
    "\n",
    "                result = outputs > threshold\n",
    "                # Measure model performance for every batch\n",
    "                f1_score, recall, precision, hamming_loss, ham_score, partial_accuracy, label_wise_accuracy =\\\n",
    "                      eval_scores(labels.cpu().detach().numpy(), result.cpu().detach().numpy(), loss.item(), print_out=True, epoch=epoch+1, batch=i)\n",
    "            \n",
    "            # Evaluation metrics for every epoch\n",
    "                \n",
    "            threshold = validate(model, val_data, epoch, model_id, loss_fn, threshold)\n",
    "        test(model, test_data, model_id, loss_fn, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Train the model \"\"\"\n",
    "\n",
    "fit(model, train_data, val_data, optimizer, loss, NUM_EPOCHS, MODEL_ID, CLASSIFICATION_THRESHOLD_VECTOR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "face_recognition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
